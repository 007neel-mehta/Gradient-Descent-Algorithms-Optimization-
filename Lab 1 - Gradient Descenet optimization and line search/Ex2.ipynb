{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190008_IE684_Lab1_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\r\n",
        "\r\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1-10)^2 + (x_2+2)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        " \\begin{array}{l}\r\n",
        "\\ We\\ can\\ write\\ the\\ function\\ f( x) \\ in\\ the\\ following\\ form\\\\\r\n",
        "\\\\\r\n",
        "f( x) \\ =\\ x^{T} Ax\\ +\\ 2b^{T} x\\ +c\\ \\\\\r\n",
        "\\\\\r\n",
        "where\\\\\r\n",
        "\\\\\r\n",
        "x\\ =\\ \\begin{bmatrix}\r\n",
        "x_{1}\\\\\r\n",
        "x_{2}\r\n",
        "\\end{bmatrix} ,\\ \\ A\\ =\\ \\begin{bmatrix}\r\n",
        "1 & 0\\\\\r\n",
        "0 & 1\r\n",
        "\\end{bmatrix} \\ ,\\ b\\ =\\ \\begin{bmatrix}\r\n",
        "-10\\\\\r\n",
        "2\r\n",
        "\\end{bmatrix} \\ ,\\ c\\ =\\ 104\r\n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\r\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uai5eDg7Pw-g"
      },
      "source": [
        " \\begin{array}{l}\r\n",
        "Suppose\\ that\\ we\\ are\\ at\\ iteration\\ k\\ with\\ the\\ value\\ x_{k} \\ and\\ we\\ define\\ our\\ descent\\ step\\\\\r\n",
        "x_{k+1} =\\ x_{k} -\\eta \\nabla f( x_{k}) \\ ,\\ \\eta  >0\\\\\r\n",
        "If\\ we\\ substitute\\ this\\ value\\ of\\ x_{k+1} \\ in\\ the\\ function\\ f( x) \\ =\\ x^{T} Ax\\ +2b^{T} x+c\\ ,then\\\\\r\n",
        "we\\ get\\ a\\ second\\ order\\ degree\\ polynomial\\ in\\ \\eta ,\\ say\\ h( \\eta ) .\\ We\\ can\\ then\\ find\\ the\\ \\\\\r\n",
        "minimum\\ of\\ this\\ function\\ by\\ finding\\ the\\ solution\\ to\\ the\\ equation\\ h'( \\eta ) \\ =0\\ and\\ we\\ get\\\\\r\n",
        "\\\\\r\n",
        "\\eta \\ =\\ \\frac{( \\nabla f( x_{k}))^{T} \\ \\nabla f( x_{k})}{( \\nabla f( x_{k}))^{T} \\ 2A \\ \\nabla f( x_{k})}\\\\\r\n",
        "\\end{array}\r\n",
        "\r\n",
        "\r\n",
        " \\begin{array}{l}\r\n",
        "Here\\ we\\ have\\ \\nabla f( x) \\ =\\ g( x) \\ =\\ \\begin{bmatrix}\r\n",
        "2( x_{1} -10)\\\\\r\n",
        "2( x_{2} +2)\r\n",
        "\\end{bmatrix}\\\\\r\n",
        "A\\ =\\ \\begin{bmatrix}\r\n",
        "1 & 0\\\\\r\n",
        "0 & 1\r\n",
        "\\end{bmatrix}\\\\\r\n",
        "We\\ get,\\ \\ \\eta \\ =\\ \\frac{\\begin{bmatrix}\r\n",
        "2( x_{1} -10)\\\\\r\n",
        "2( x_{2} +2)\r\n",
        "\\end{bmatrix}^{T}\\begin{bmatrix}\r\n",
        "2( x_{1} -10)\\\\\r\n",
        "2( x_{2} +2)\r\n",
        "\\end{bmatrix}}{\\begin{bmatrix}\r\n",
        "2( x_{1} -10)\\\\\r\n",
        "2( x_{2} +2)\r\n",
        "\\end{bmatrix}^{T} 2A\\begin{bmatrix}\r\n",
        "2( x_{1} -10)\\\\\r\n",
        "2( x_{2} +2)\r\n",
        "\\end{bmatrix}} \\ =\\ 0.5\\\\\r\n",
        "\\\\\r\n",
        "Hence\\ we\\ observe\\ that\\ the\\ \\eta \\ does\\ not\\ depend\\ on\\ the\\ initial\\ value\\ of\\ the\\ x\\ and\\ hence\\ is\\ constant.\r\n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \r\n",
        "\r\n",
        "\r\n",
        "\\begin{align}\r\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\r\n",
        "& \\textbf{Initialize } k=0 \\\\ \r\n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \r\n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\r\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \r\n",
        "&\\quad \\quad k = {k+1} \\\\ \r\n",
        "&\\textbf{End While} \\\\\r\n",
        "&\\textbf{Output: } \\mathbf{x}^k\r\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/1.19/ for numpy documentation\r\n",
        "#we will first import the numpy package and name it as np\r\n",
        "import numpy as np \r\n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \r\n",
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  return (x[0]-10)**2 + (x[1]+2)**2\r\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \r\n",
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  return np.array([2*(x[0]-10),2*(x[1]+2)])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\r\n",
        "def compute_steplength(g_vector, a_matrix):\r\n",
        "\r\n",
        "  assert a_matrix.shape[0] == len(g_vector)\r\n",
        "  assert a_matrix.shape[1] == len(g_vector)\r\n",
        "  step_length = (np.dot(g_vector.T, g_vector)) / (np.matmul(np.matmul(g_vector.T,2*a_matrix ),g_vector))\r\n",
        " \r\n",
        "  return step_length"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "  k = 0\r\n",
        "  a_matrix = np.array([\r\n",
        "                       [1,0],\r\n",
        "                       [0,1]\r\n",
        "                       ])\r\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "    step_length = compute_steplength(g_x,a_matrix) #call the new function you wrote to compute the steplength\r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x,k \r\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4utHmL3XqTr2"
      },
      "source": [
        "# Ex2. Part3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3f3109-55e8-49b5-838e-54797bd55265"
      },
      "source": [
        "my_start_x = np.array([0,0])\r\n",
        "my_tol= 1e-3\r\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 0  x: [0 0]  f(x): 104  grad at x: [-20   4]  gradient norm: 20.396078054371138\n",
            "iter: 1  x: [10. -2.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([10., -2.]), 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OB1hpeoqb_-"
      },
      "source": [
        "# Part 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_3BxMzfREx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "0239d17c-2900-45b7-80cf-62e1e2d6ce12"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "my_start_x = np.array([0,0])\r\n",
        "tol_arr = (10**(-np.linspace(1,12,12))).tolist()\r\n",
        "\r\n",
        "opt_x = []\r\n",
        "opt_fx = []\r\n",
        "iteration = []\r\n",
        "\r\n",
        "for i in tol_arr:\r\n",
        "  a,b = find_minimizer(my_start_x,i)\r\n",
        "  opt_x.append(a)\r\n",
        "  opt_fx.append(evalf(a))\r\n",
        "  iteration.append(b)\r\n",
        "\r\n",
        "\r\n",
        "old_iterations = [24,35,45,55,66,76,86,97,107,117,128,138]\r\n",
        "\r\n",
        "plt.plot(tol_arr,iteration, label ='Exact Line Search')\r\n",
        "plt.plot(tol_arr,old_iterations, label='Fixed Step Length')\r\n",
        "plt.xlabel('Tollerance')\r\n",
        "plt.ylabel('No. of iterations performed')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "print('\\n\\n')\r\n",
        "df = pd.DataFrame(np.array(opt_x), columns = ['Optimal_x1', 'Optimal_x2'])\r\n",
        "df['Tolerance Value'] = tol_arr\r\n",
        "df['Optimal_f(x)'] = opt_fx\r\n",
        "df['number_of_iterations'] = iteration\r\n",
        "df[['Tolerance Value', 'Optimal_x1', 'Optimal_x2', 'Optimal_f(x)', 'number_of_iterations']]\r\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tolerance Value</th>\n",
              "      <th>Optimal_x1</th>\n",
              "      <th>Optimal_x2</th>\n",
              "      <th>Optimal_f(x)</th>\n",
              "      <th>number_of_iterations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000e-01</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000000e-02</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000000e-03</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.000000e-04</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000000e-05</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000000e-06</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.000000e-07</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.000000e-10</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.000000e-11</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.000000e-12</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Tolerance Value  Optimal_x1  Optimal_x2  Optimal_f(x)  number_of_iterations\n",
              "0      1.000000e-01        10.0        -2.0           0.0                     1\n",
              "1      1.000000e-02        10.0        -2.0           0.0                     1\n",
              "2      1.000000e-03        10.0        -2.0           0.0                     1\n",
              "3      1.000000e-04        10.0        -2.0           0.0                     1\n",
              "4      1.000000e-05        10.0        -2.0           0.0                     1\n",
              "5      1.000000e-06        10.0        -2.0           0.0                     1\n",
              "6      1.000000e-07        10.0        -2.0           0.0                     1\n",
              "7      1.000000e-08        10.0        -2.0           0.0                     1\n",
              "8      1.000000e-09        10.0        -2.0           0.0                     1\n",
              "9      1.000000e-10        10.0        -2.0           0.0                     1\n",
              "10     1.000000e-11        10.0        -2.0           0.0                     1\n",
              "11     1.000000e-12        10.0        -2.0           0.0                     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgAkpaNYx3c-"
      },
      "source": [
        "***Comment :*** *We observe from the above graph that the number of iterations taken by the algorith when the step length was fixed, is higher than that in the algo where we use line search method. When we update the step length after every iteration, the optimal solution is achieved faster.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [
        ""
      ],
      "execution_count": 69,
      "outputs": []
    }
  ]
}