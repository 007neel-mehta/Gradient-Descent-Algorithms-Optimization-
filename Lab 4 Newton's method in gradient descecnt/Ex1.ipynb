{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190008_IE684_Lab4_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fyEzu8JbysxV",
        "G0ajeZVpyqrB",
        "Hicmf3BS7Hvz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tit3N_Zucufg"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezg5FeXlhMsj"
      },
      "source": [
        "$f(x) = 100x_1^2 + 0.001x_2^4$\r\n",
        "\r\n",
        "We will use the newton's method to solve for the problem, $\\min f(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyEzu8JbysxV"
      },
      "source": [
        "# General Code for the given problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUBzD9jFdwdV"
      },
      "source": [
        "def evalf(x):  \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 \r\n",
        "  return 100*x[0]**2 + 0.001*x[1]**4\r\n",
        "  \r\n",
        "def evalg(x):  \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2\r\n",
        "  return np.array([200*x[0], 0.004*x[1]**3 ])\r\n",
        "\r\n",
        "def evalh(x):\r\n",
        "  assert type(x) is np.ndarray \r\n",
        "  assert len(x) == 2\r\n",
        "  return np.array([\r\n",
        "                   [200, 0],\r\n",
        "                   [0, 0.012*x[1]**2]\r\n",
        "                   ])\r\n",
        "\r\n",
        "\r\n",
        "#Defining a function which calculated the matrix Dk. If the second parameter is true then it calculate the diagonal matrix Dk by taking only the digonal inverse of H, and calculate the H inverse if its true. (Psedo inverse if det of H is zero)\r\n",
        "def compute_D_k(x, diagonal_only = False):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == 2\r\n",
        "\r\n",
        "  if diagonal_only == False:\r\n",
        "\r\n",
        "    if np.linalg.det(evalh(x)) == 0:\r\n",
        "      print('The determinant of the hessian is zero. Hence we find Pseudo Inverse')\r\n",
        "      return np.linalg.pinv(evalh(x))\r\n",
        "    else:\r\n",
        "      return np.linalg.inv(evalh(x))\r\n",
        "\r\n",
        "  else:\r\n",
        "    return np.array([\r\n",
        "                   [1/(evalh(x)[0][0]),0],\r\n",
        "                   [0, 1/(evalh(x)[1][1])]\r\n",
        "                   ])\r\n",
        "\r\n",
        "\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "\r\n",
        "  alpha = alpha_start\r\n",
        "  p = -gradf\r\n",
        "\r\n",
        "  while (evalf(x + alpha*p) > (evalf(x) + gamma * alpha * np.dot(gradf.T, p)) ):\r\n",
        "    alpha = alpha*rho\r\n",
        "  \r\n",
        "  return alpha\r\n",
        "\r\n",
        "\r\n",
        "def compute_steplength_backtracking_scaled_direction(x, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \r\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "\r\n",
        "  alpha = alpha_start\r\n",
        "  p = -gradf\r\n",
        "\r\n",
        "  while (evalf(x + alpha*np.matmul(direction,p)) > (evalf(x) + gamma * alpha * np.matmul(gradf.T, np.matmul(direction, p))) ):\r\n",
        "    alpha = alpha*rho\r\n",
        "  \r\n",
        "  return alpha\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "#line search type \r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3\r\n",
        "  \r\n",
        "  \r\n",
        "#complete the code for gradient descent to find the minimizer\r\n",
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "\r\n",
        "  #A = np.array([])\r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "  k = 0\r\n",
        "  \r\n",
        "\r\n",
        "  if (line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2] \r\n",
        "\r\n",
        "  while np.linalg.norm(g_x) > tol:\r\n",
        "\r\n",
        "    if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_exact(g_x,A)\r\n",
        "\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma)\r\n",
        "\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: \r\n",
        "      step_length = float(input('Enter a valid value for the constant step length.'))\r\n",
        "\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "\r\n",
        "    \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) \r\n",
        "    k += 1 \r\n",
        "    g_x = evalg(x)\r\n",
        "\r\n",
        "  return x, k, \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,diagonal_Dk, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        " \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "  k = 0\r\n",
        "  \r\n",
        " \r\n",
        "  if (line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2] \r\n",
        "    \r\n",
        " \r\n",
        "  while np.linalg.norm(g_x) > tol:\r\n",
        " \r\n",
        "    #if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      #step_length = #step_length using exact line search with sclaing\r\n",
        " \r\n",
        "    d = compute_D_k(x, diagonal_Dk)\r\n",
        "    \r\n",
        "\r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x, d , alpha_start,rho, gamma)\r\n",
        " \r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: \r\n",
        "      step_length = 1.0\r\n",
        " \r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        " \r\n",
        "    x = np.subtract(x, step_length * np.matmul(d,g_x)) \r\n",
        "    k += 1 \r\n",
        "    g_x = evalg(x)\r\n",
        " \r\n",
        "  return x, k\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ajeZVpyqrB"
      },
      "source": [
        "# ***Part 3 :***  *Comparing the newton's method for a constant value of $\\eta $, with the one where $\\eta$ is calculated using Backtracking Line search*\r\n",
        "\r\n",
        "$\\eta^k =1.0 \\ \\ \\  \\forall \\ k = 1,2,3 ... $\r\n",
        "\r\n",
        "$x^0 = (1.0, 1.0)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IRjLj6wx0U1",
        "outputId": "95c62d1b-897f-46ea-9f02-c5e3587b5c20"
      },
      "source": [
        "start_x = np.array([1.0, 1.0])\r\n",
        "tol = 10e-9\r\n",
        "alpha = 1.0\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "\r\n",
        "x_constant, k_constant = find_minimizer_gdscaling(start_x,tol,CONSTANT_STEP_LENGTH,False,alpha, rho, gamma)\r\n",
        "x_bls_scaling, k_bls_scaling = find_minimizer_gdscaling(start_x,tol, BACKTRACKING_LINE_SEARCH,False,alpha,rho, gamma)\r\n",
        "\r\n",
        "print(\"Newton's method with Constant eta :-\", end='\\n')\r\n",
        "print('Optimal X :', x_constant)\r\n",
        "print('Optimal value of f(x) :', evalf(x_constant))\r\n",
        "print('Number of Iterations :', k_constant)\r\n",
        "\r\n",
        "print('\\n\\n')\r\n",
        "\r\n",
        "print(\"Newton's method with BLS :-\", end='\\n')\r\n",
        "print('Optimal X :', x_bls_scaling)\r\n",
        "print('Optimal value of f(x) :', evalf(x_bls_scaling))\r\n",
        "print('Number of Iterations :', k_bls_scaling)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newton's method with Constant eta :-\n",
            "Optimal X : [0.         0.01156102]\n",
            "Optimal value of f(x) : 1.7864242338403204e-11\n",
            "Number of Iterations : 11\n",
            "\n",
            "\n",
            "\n",
            "Newton's method with BLS :-\n",
            "Optimal X : [0.         0.01156102]\n",
            "Optimal value of f(x) : 1.7864242338403204e-11\n",
            "Number of Iterations : 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzK5Xsf-6uD7"
      },
      "source": [
        "***Remarks :*** *We observe that the the optimal value of X, the function and the number of iterations is all same in both the case, wether we use backtraking line search or we use constant method to find the value of the step length, $\\eta$. Because even when we use the BLS method the hessian matrix inverse turn out to be such that the step length is 1.0. Also the hessian matrix is alredy a diagonal matix so Dk does not change*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hicmf3BS7Hvz"
      },
      "source": [
        "# ***Part 4 :***  *Comparing the above two methods with BLS(without scaling) and BLS(with scaling with diagonal $D^k$)*\r\n",
        "\r\n",
        "$\\eta^k =1.0 \\ \\ \\  \\forall \\ k = 1,2,3 ... $\r\n",
        "\r\n",
        "$x^0 = (1.0, 1.0)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbQByTYcz7um"
      },
      "source": [
        "x_bls, k_bls = find_minimizer_gd(start_x,tol,BACKTRACKING_LINE_SEARCH,alpha, rho, gamma)\r\n",
        "#x_bls_diag, k_bls_diag = find_minimizer_gdscaling(start_x,tol,BACKTRACKING_LINE_SEARCH,True,alpha, rho, gamma)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ma2cCob-7pl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "bf05cebe-408d-4f5f-bf7a-cf67e2e6455d"
      },
      "source": [
        "df = pd.DataFrame(columns=['Method', 'Optimal_X', 'Optimal_f(x)', 'Iterations'])\r\n",
        "\r\n",
        "df['Method'] = ['Constant Step Lenth, eta =1.0 ', 'BLS with scaling', 'BLS without Scaling', 'BLS with scaling with Diagonal Dk']\r\n",
        "df['Optimal_X'] = [x_constant, x_bls_scaling, x_bls, x_bls_diag]\r\n",
        "df['Optimal_f(x)'] = [evalf(x_constant), evalf(x_bls_scaling), evalf(x_bls), evalf(x_bls_diag)]\r\n",
        "df['Iterations'] = [k_constant, k_bls_scaling, k_bls, k_bls_diag]\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Optimal_X</th>\n",
              "      <th>Optimal_f(x)</th>\n",
              "      <th>Iterations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Constant Step Lenth, eta =1.0</td>\n",
              "      <td>[0.0, 0.011561019943888407]</td>\n",
              "      <td>1.786424e-11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BLS with scaling</td>\n",
              "      <td>[0.0, 0.011561019943888407]</td>\n",
              "      <td>1.786424e-11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BLS without Scaling</td>\n",
              "      <td>[1.4119627465647396e-11, 0.013385327284182453]</td>\n",
              "      <td>3.210081e-11</td>\n",
              "      <td>48818419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BLS with scaling with Diagonal Dk</td>\n",
              "      <td>[0.0, 0.011561019943888407]</td>\n",
              "      <td>1.786424e-11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Method  ... Iterations\n",
              "0     Constant Step Lenth, eta =1.0   ...         11\n",
              "1                   BLS with scaling  ...         11\n",
              "2                BLS without Scaling  ...   48818419\n",
              "3  BLS with scaling with Diagonal Dk  ...         11\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp3-INqXRHGE"
      },
      "source": [
        "***Remarks :*** *We observe that the number of iterations and the optimal value is same for all the cases except for the BLS without scaling method, in which it took more than 4.88 crore iterations to reach to the optimal value. The optimal value of the X is (0, 0.011561) which is very closed achieved by BLS without scaling.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGBLHKlTRqFU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}