{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190008_IE684_Lab4_Ex2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fyEzu8JbysxV",
        "G0ajeZVpyqrB",
        "Hicmf3BS7Hvz",
        "j6YN3vmPUdTR",
        "uS0T9PvTUueS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ26MnUWGBsO"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezg5FeXlhMsj"
      },
      "source": [
        "$f(x) = \\sqrt{x_1^2+1} + \\sqrt{x_2^2 +1}$\r\n",
        "\r\n",
        "We will use the newton's method to solve for the problem, $\\min f(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyEzu8JbysxV"
      },
      "source": [
        "# General Code for the given problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xZ_NCe5GpTQ"
      },
      "source": [
        "def evalf(x):  \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 \r\n",
        "  return (x[0]**2 + 1 )**0.5 + (x[1]**2 + 1 )**0.5 \r\n",
        "  \r\n",
        "def evalg(x):  \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2\r\n",
        "  return np.array([ (x[0]) / ((x[0]**2 + 1 )**0.5) ,  (x[1]) / ((x[1]**2 + 1 )**0.5) ])\r\n",
        "\r\n",
        "def evalh(x):\r\n",
        "  assert type(x) is np.ndarray \r\n",
        "  assert len(x) == 2\r\n",
        "  return np.array([\r\n",
        "                   [ 1 / (x[0]**2 + 1)**1.5 , 0],\r\n",
        "                   [0, 1 / (x[1]**2 + 1)**1.5]\r\n",
        "                   ])\r\n",
        "\r\n",
        "\r\n",
        "#Defining a function which calculated the matrix Dk. If the second parameter is true then it calculate the diagonal matrix Dk by taking only the digonal inverse of H, and calculate the H inverse if its true. (Psedo inverse if det of H is zero)\r\n",
        "def compute_D_k(x, diagonal_only = False):\r\n",
        "  assert type(x) is np.ndarray\r\n",
        "  assert len(x) == 2\r\n",
        "\r\n",
        "  if diagonal_only == False:\r\n",
        "\r\n",
        "    if np.linalg.det(evalh(x)) == 0:\r\n",
        "      print('The determinant of the hessian is zero. Hence we find Pseudo Inverse')\r\n",
        "      return np.linalg.pinv(evalh(x))\r\n",
        "    else:\r\n",
        "      return np.linalg.inv(evalh(x))\r\n",
        "\r\n",
        "  else:\r\n",
        "    return np.array([\r\n",
        "                   [1/(evalh(x)[0][0]),0],\r\n",
        "                   [0, 1/(evalh(x)[1][1])]\r\n",
        "                   ])\r\n",
        "\r\n",
        "\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "\r\n",
        "  alpha = alpha_start\r\n",
        "  p = -gradf\r\n",
        "\r\n",
        "  while (evalf(x + alpha*p) > (evalf(x) + gamma * alpha * np.dot(gradf.T, p)) ):\r\n",
        "    alpha = alpha*rho\r\n",
        "  \r\n",
        "  return alpha\r\n",
        "\r\n",
        "\r\n",
        "def compute_steplength_backtracking_scaled_direction(x, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \r\n",
        "  assert type(alpha_start) is float and alpha_start>=0. \r\n",
        "  assert type(rho) is float and rho>=0.\r\n",
        "  assert type(gamma) is float and gamma>=0. \r\n",
        "\r\n",
        "  alpha = alpha_start\r\n",
        "  p = -gradf\r\n",
        "\r\n",
        "  while (evalf(x + alpha*np.matmul(direction,p)) > (evalf(x) + gamma * alpha * np.matmul(gradf.T, np.matmul(direction, p))) ):\r\n",
        "    alpha = alpha*rho\r\n",
        "  \r\n",
        "  return alpha\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "#line search type \r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3\r\n",
        "  \r\n",
        "  \r\n",
        "#complete the code for gradient descent to find the minimizer\r\n",
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "\r\n",
        "  #A = np.array([])\r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "  k = 0\r\n",
        "  \r\n",
        "\r\n",
        "  if (line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2] \r\n",
        "\r\n",
        "  while np.linalg.norm(g_x) > tol:\r\n",
        "\r\n",
        "    if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_exact(g_x,A)\r\n",
        "\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma)\r\n",
        "\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: \r\n",
        "      step_length = float(input('Enter a valid value for the constant step length.'))\r\n",
        "\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "\r\n",
        "    \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) \r\n",
        "    k += 1 \r\n",
        "    g_x = evalg(x)\r\n",
        "\r\n",
        "  return x, k, \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,diagonal_Dk, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        " \r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "  k = 0\r\n",
        "  \r\n",
        " \r\n",
        "  if (line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2] \r\n",
        "    \r\n",
        " \r\n",
        "  while np.linalg.norm(g_x) > tol:\r\n",
        " \r\n",
        "    #if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      #step_length = #step_length using exact line search with sclaing\r\n",
        " \r\n",
        "    d = compute_D_k(x, diagonal_Dk)\r\n",
        "    \r\n",
        "\r\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x, d , alpha_start,rho, gamma)\r\n",
        " \r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: \r\n",
        "      step_length = 0.9999\r\n",
        " \r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        " \r\n",
        "    x = np.subtract(x, step_length * np.matmul(d,g_x)) \r\n",
        "    k += 1 \r\n",
        "    g_x = evalg(x)\r\n",
        " \r\n",
        "  return x, k\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ajeZVpyqrB"
      },
      "source": [
        "# ***Part 2 :***  *Comparing the newton's method for a constant value of $\\eta $, with the one where $\\eta$ is calculated using Backtracking Line search*\r\n",
        "\r\n",
        "$\\eta^k =1.0 \\ \\ \\  \\forall \\ k = 1,2,3 ... $\r\n",
        "\r\n",
        "$x^0 = (1.0, 1.0)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgode4vTL-ze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5bc57b-fd58-44dd-c9bf-b33dddbf913a"
      },
      "source": [
        "start_x = np.array([1.0, 1.0])\r\n",
        "tol = 10e-9\r\n",
        "alpha = 1.0\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "\r\n",
        "x_constant, k_constant = find_minimizer_gdscaling(start_x,tol,CONSTANT_STEP_LENGTH,False,alpha, rho, gamma)\r\n",
        "x_bls_scaling, k_bls_scaling = find_minimizer_gdscaling(start_x,tol, BACKTRACKING_LINE_SEARCH,False,alpha,rho, gamma)\r\n",
        "\r\n",
        "print(\"Newton's method with Constant eta :-\", end='\\n')\r\n",
        "print('Optimal X :', x_constant)\r\n",
        "print('Optimal value of f(x) :', evalf(x_constant))\r\n",
        "print('Number of Iterations :', k_constant)\r\n",
        "\r\n",
        "print('\\n\\n')\r\n",
        "\r\n",
        "print(\"Newton's method with BLS :-\", end='\\n')\r\n",
        "print('Optimal X :', x_bls_scaling)\r\n",
        "print('Optimal value of f(x) :', evalf(x_bls_scaling))\r\n",
        "print('Number of Iterations :', k_bls_scaling)\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newton's method with Constant eta :-\n",
            "Optimal X : [2.50433338e-11 2.50433338e-11]\n",
            "Optimal value of f(x) : 2.0\n",
            "Number of Iterations : 12\n",
            "\n",
            "\n",
            "\n",
            "Newton's method with BLS :-\n",
            "Optimal X : [0. 0.]\n",
            "Optimal value of f(x) : 2.0\n",
            "Number of Iterations : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK4h-l9m5NnC"
      },
      "source": [
        "***Remarks :*** *We observe that the optimal answer for the problem is found when the constant step length is 0.999 instead of 1.0, because at 1.0 the solution is not converging and the algorith is overshooting the value and oscillating. Hene we report the optimal X, the optimal value of the function and the number of iteration taken by the algorithm*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hicmf3BS7Hvz"
      },
      "source": [
        "# ***Part 3 :***  *Comparing the above two methods with BLS(without scaling)*\r\n",
        "\r\n",
        "$\\eta^k =1.0 \\ \\ \\  \\forall \\ k = 1,2,3 ... $\r\n",
        "\r\n",
        "$x^0 = (1.0, 1.0)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkgPhshVMSfm"
      },
      "source": [
        "x_bls, k_bls = find_minimizer_gd(start_x,tol,BACKTRACKING_LINE_SEARCH,alpha, rho, gamma)\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u31VjGHhMaUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "4f2661e8-7c33-492e-b687-05ff87cb5262"
      },
      "source": [
        "df = pd.DataFrame(columns=['Method', 'Optimal_X', 'Optimal_f(x)', 'Iterations'])\r\n",
        "\r\n",
        "df['Method'] = ['Constant Step Lenth, eta =1.0 ', 'BLS with scaling', 'BLS without Scaling']\r\n",
        "df['Optimal_X'] = [x_constant, x_bls_scaling, x_bls]\r\n",
        "df['Optimal_f(x)'] = [evalf(x_constant), evalf(x_bls_scaling), evalf(x_bls)]\r\n",
        "df['Iterations'] = [k_constant, k_bls_scaling, k_bls]\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Optimal_X</th>\n",
              "      <th>Optimal_f(x)</th>\n",
              "      <th>Iterations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Constant Step Lenth, eta =1.0</td>\n",
              "      <td>[2.50433337718496e-11, 2.50433337718496e-11]</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BLS with scaling</td>\n",
              "      <td>[0.0, 0.0]</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BLS without Scaling</td>\n",
              "      <td>[2.7899147700188517e-19, 2.7899147700188517e-19]</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Method  ... Iterations\n",
              "0  Constant Step Lenth, eta =1.0   ...         12\n",
              "1                BLS with scaling  ...          1\n",
              "2             BLS without Scaling  ...          4\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6YN3vmPUdTR"
      },
      "source": [
        "# ***Part 4 :***  *Comparing the newton's method for a constant value of $\\eta $, with the one where $\\eta$ is calculated using Backtracking Line search*\r\n",
        "\r\n",
        "$\\eta^k =1.0 \\ \\ \\  \\forall \\ k = 1,2,3 ... $\r\n",
        "\r\n",
        "$x^0 = (10.0, 10.0)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGcg2UxsUgwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba098116-263e-4420-a243-c680f124bab3"
      },
      "source": [
        "start_x = np.array([10.0, 10.0])\r\n",
        "tol = 10e-9\r\n",
        "alpha = 1.0\r\n",
        "rho = 0.5\r\n",
        "gamma = 0.5\r\n",
        "\r\n",
        "x_constant, k_constant = find_minimizer_gdscaling(start_x,tol,CONSTANT_STEP_LENGTH,False,alpha, rho, gamma)\r\n",
        "x_bls_scaling, k_bls_scaling = find_minimizer_gdscaling(start_x,tol, BACKTRACKING_LINE_SEARCH,False,alpha,rho, gamma)\r\n",
        "\r\n",
        "print(\"Newton's method with Constant eta :-\", end='\\n')\r\n",
        "print('Optimal X :', x_constant)\r\n",
        "print('Optimal value of f(x) :', evalf(x_constant))\r\n",
        "print('Number of Iterations :', k_constant)\r\n",
        "\r\n",
        "print('\\n\\n')\r\n",
        "\r\n",
        "print(\"Newton's method with BLS :-\", end='\\n')\r\n",
        "print('Optimal X :', x_bls_scaling)\r\n",
        "print('Optimal value of f(x) :', evalf(x_bls_scaling))\r\n",
        "print('Number of Iterations :', k_bls_scaling)\r\n",
        "\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The determinant of the hessian is zero. Hence we find Pseudo Inverse\n",
            "Newton's method with Constant eta :-\n",
            "Optimal X : [-9.8789228e+242 -9.8789228e+242]\n",
            "Optimal value of f(x) : inf\n",
            "Number of Iterations : 5\n",
            "\n",
            "\n",
            "\n",
            "Newton's method with BLS :-\n",
            "Optimal X : [-9.92761578e-15 -9.92761578e-15]\n",
            "Optimal value of f(x) : 2.0\n",
            "Number of Iterations : 17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in double_scalars\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS0T9PvTUueS"
      },
      "source": [
        "# ***Part 5 :***  *Comparing the above two methods with BLS(without scaling)*\r\n",
        "\r\n",
        "$\\eta^k =1.0 \\ \\ \\  \\forall \\ k = 1,2,3 ... $\r\n",
        "\r\n",
        "$x^0 = (10.0, 10.0)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnl3RPHIUueX"
      },
      "source": [
        "x_bls, k_bls = find_minimizer_gd(start_x,tol,BACKTRACKING_LINE_SEARCH,alpha, rho, gamma)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2n2A8jVUueY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "a6381800-12ae-480a-aa0d-e7759660625f"
      },
      "source": [
        "df = pd.DataFrame(columns=['Method', 'Optimal_X', 'Optimal_f(x)', 'Iterations'])\r\n",
        "\r\n",
        "df['Method'] = ['Constant Step Lenth, eta =1.0 ', 'BLS with scaling', 'BLS without Scaling']\r\n",
        "df['Optimal_X'] = [x_constant, x_bls_scaling, x_bls]\r\n",
        "df['Optimal_f(x)'] = [evalf(x_constant), evalf(x_bls_scaling), evalf(x_bls)]\r\n",
        "df['Iterations'] = [k_constant, k_bls_scaling, k_bls]\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Optimal_X</th>\n",
              "      <th>Optimal_f(x)</th>\n",
              "      <th>Iterations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Constant Step Lenth, eta =1.0</td>\n",
              "      <td>[-9.878922796403032e+242, -9.878922796403032e+...</td>\n",
              "      <td>inf</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BLS with scaling</td>\n",
              "      <td>[-9.927615776976137e-15, -9.927615776976137e-15]</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BLS without Scaling</td>\n",
              "      <td>[2.1245585314893373e-14, 2.1245585314893373e-14]</td>\n",
              "      <td>2.0</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Method  ... Iterations\n",
              "0  Constant Step Lenth, eta =1.0   ...          5\n",
              "1                BLS with scaling  ...         17\n",
              "2             BLS without Scaling  ...         13\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye_pBgVe7xwm"
      },
      "source": [
        "***Remarks :*** *In part 4 and 5 we observe that for the initian value of x = (10,10) the value of the determinant of the hessian is zero, thats why we see absurd answer which is converging at inf. But for other two cases, where the constant step length is 0.999, the answer if converging in both BLS with scaling and without scaling. And we can see the optimal value of the x, f(x) and the number of iteration in the above table.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc8mnmbq8Ow1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}