{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20i190008_IE684_Lab2_Ex1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nk7caK8lfd0_",
        "jxLizcLJ5Xz_",
        "R4Ia6XSt6gR4",
        "XRulPb3pKBA7",
        "afOx1Aq999jJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 2. Exercise 1. }$\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "We saw that for quadratic functions, a closed form analytical solution for the minimizer of the optimization problem $\\min_{\\eta \\geq 0} f({\\mathbf{x}}^k + \\eta {\\mathbf{p}}^k)$ exists. However finding a closed form expression as a solution to this optimization problem to find a suitable step length might not always be possible. To tackle general situations, we will try to devise a different procedure in this lab. \r\n",
        "\r\n",
        "To find the step length, we will use the following property: \r\n",
        "Suppose a non-zero $\\mathbf{p} \\in {\\mathbb{R}}^n$ is a descent direction at point $\\mathbf{x}$, and let $\\gamma \\in (0,1)$. Then there exists $\\varepsilon >0$ such that  \r\n",
        "\\begin{align}\r\n",
        "f(\\mathbf{x}+\\alpha \\mathbf{p}) \\leq f(\\mathbf{x}) + \\gamma \\alpha \\nabla f(\\mathbf{x})^\\top \\mathbf{p}, \\ \\forall \\alpha \\in (0,\\varepsilon].  \r\n",
        "\\end{align}\r\n",
        "\r\n",
        "The step length $\\eta^k$ can be found using a backtracking procedure illustrated below to find appropriate value of $\\varepsilon$.  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6OddaNAmpA"
      },
      "source": [
        "\r\n",
        "\\begin{align}\r\n",
        "& \\textbf{Input:}  \\text{ $\\mathbf{x}^k$, $\\mathbf{p}^k$, $\\alpha^0$, $\\rho \\in (0,1)$, $\\gamma \\in (0,1)$ }  \\\\\r\n",
        "& \\textbf{Initialize } \\alpha=\\alpha^0 \\\\ \r\n",
        "&\\textbf{While } f(\\mathbf{x}^k + \\alpha \\mathbf{p}^k)   > f(\\mathbf{x}^k) + \\gamma \\alpha \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{p}^k \\text{ do:}  \\\\   \r\n",
        "&\\quad \\quad \\alpha = \\rho \\alpha  \\\\\r\n",
        "&\\textbf{End While} \\\\\r\n",
        "&\\textbf{Output: } \\alpha\r\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-xcDISWmGR"
      },
      "source": [
        "In this exercise, we will check if finding the steplength using the backtracking procedure is advantageous for some quadratic functions. In this sample code we consider $f(\\mathbf{x})=f(x_1,x_2) = (x_1-2)^2 + (x_2 + 3)^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk7caK8lfd0_"
      },
      "source": [
        "# ***Part 1 and 2:*** *General code and definitions* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/1.19/ for numpy documentation\r\n",
        "#we will first import the numpy package and name it as np\r\n",
        "import numpy as np \r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \r\n",
        "def evalf(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the objective function value\r\n",
        "  #compute the function value and return it \r\n",
        "  return (x[1]+3)**2 + (-2+x[0])**2\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \r\n",
        "def evalg(x):  \r\n",
        "  #Input: x is a numpy array of size 2 \r\n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \r\n",
        "  #after checking if the argument is valid, we can compute the gradient value\r\n",
        "  #compute the gradient value and return it \r\n",
        "  return np.array([2*(x[0]-2), 2*(x[1]+3)])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\r\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\r\n",
        "   \r\n",
        "  step_length = (np.dot(gradf.T, gradf)) / (np.matmul(np.matmul(gradf.T,2*A ),gradf))\r\n",
        "  \r\n",
        "  return step_length"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\r\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \r\n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \r\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \r\n",
        "  \r\n",
        "  alpha = alpha_start\r\n",
        "  p = -gradf\r\n",
        "\r\n",
        "  while (evalf(x + alpha*p) > (evalf(x) + gamma * alpha * np.dot(gradf.T, p)) ):\r\n",
        "    alpha = alpha*rho\r\n",
        "  \r\n",
        "  #print('final step length:',alpha)\r\n",
        "  return alpha"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\r\n",
        "EXACT_LINE_SEARCH = 1\r\n",
        "BACKTRACKING_LINE_SEARCH = 2\r\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\r\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\r\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \r\n",
        "  assert type(tol) is float and tol>=0 \r\n",
        "  # construct a suitable A matrix for the quadratic function \r\n",
        "  A = np.array([[1, 0],[0,1]])\r\n",
        "  x = start_x\r\n",
        "  g_x = evalg(x)\r\n",
        "\r\n",
        "  #initialization for backtracking line search\r\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\r\n",
        "    alpha_start = args[0]\r\n",
        "    rho = args[1]\r\n",
        "    gamma = args[2]\r\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\r\n",
        "\r\n",
        "  k = 0\r\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "\r\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\r\n",
        "  \r\n",
        "    if line_search_type == EXACT_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "\r\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\r\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\r\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\r\n",
        "\r\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\r\n",
        "      step_length = 0.1\r\n",
        "\r\n",
        "    else:  \r\n",
        "      raise ValueError('Line search type unknown. Please check!')\r\n",
        "    \r\n",
        "    #implement the gradient descent steps here   \r\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\r\n",
        "    k += 1 #increment iteration\r\n",
        "    g_x = evalg(x) #compute gradient at new point\r\n",
        "\r\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\r\n",
        "  return x,k \r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxLizcLJ5Xz_"
      },
      "source": [
        "# ***Part 3*** : *Minimizer and the minimum value for*\r\n",
        "\r\n",
        "$f( x) \\ =\\ ( x_{1} -2)^{2} +( x_{2} +3)^{2}$\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba0f72d-f5f5-49d8-ad64-ce5a25fefd72"
      },
      "source": [
        "\r\n",
        "my_start_x = np.array([5,5])\r\n",
        "my_tol= 1e-5\r\n",
        "\r\n",
        "print('Using Exact line search  :- ')\r\n",
        "x_opt, k_opt = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\r\n",
        "print('The optimal value of x1 and x2 are :',x_opt)\r\n",
        "print('The value of the function at this point :' , evalf(x_opt))\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "#check what happens when you call find_minimzer using backtracking line search\r\n",
        "print('Using Backtracking Line search :- ')\r\n",
        "x_opt_bls, k_opt_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\r\n",
        "print('The optimal value of x : ',x_opt_bls)\r\n",
        "print('The minimum value of f(x) at x :', evalf(x_opt_bls))\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Exact line search  :- \n",
            "The optimal value of x1 and x2 are : [ 2.00000151 -2.99999598]\n",
            "The value of the function at this point : 1.8408617293418483e-11\n",
            "\n",
            "\n",
            "Using Backtracking Line search :- \n",
            "The optimal value of x :  [ 2. -3.]\n",
            "The minimum value of f(x) at x : 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ia6XSt6gR4"
      },
      "source": [
        "# ***Part4:*** *Comparing exact line search with backtracking*\r\n",
        "\r\n",
        " \\begin{array}{l}\r\n",
        "\\tau \\ =\\ 10^{-12}\\\\\r\n",
        "x^{0} =\\ ( 0,10)\\\\\r\n",
        "\\\\\r\n",
        "\\alpha ^{0} \\ =1\\\\\r\n",
        "\\rho \\ =\\ 0.5\\\\\r\n",
        "\\gamma \\ =\\ 0.5\\ \r\n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c99206-dbac-4668-ee98-078e795035aa"
      },
      "source": [
        "my_start_x = np.array([0,10])\r\n",
        "my_tol= 10e-12\r\n",
        "\r\n",
        "alpha_initial = 1\r\n",
        "rho_ = 0.5\r\n",
        "gamma_ = 0.5\r\n",
        "\r\n",
        "x_opt_els, k_els = find_minimizer(my_start_x, my_tol,EXACT_LINE_SEARCH)\r\n",
        "x_opt_bls, k_bls = find_minimizer(my_start_x, my_tol,BACKTRACKING_LINE_SEARCH, alpha_initial,rho_, gamma_ )\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print('No. of iterations in Exact Line Search :', k_els)\r\n",
        "print('No. of iterations in Backtracking Line Search :', k_bls)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "\n",
            "\n",
            "No. of iterations in Exact Line Search : 1\n",
            "No. of iterations in Backtracking Line Search : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg6QdR31Dyac"
      },
      "source": [
        "***Remarks :*** *We observe that in both the cases the number of iteration is just 1. In the ELS, we minimised a quardratic function to solve for the optimal value of step size that minimize f(x). Since this function was quadratic it was easy to calculate such value in closed form. In BLS we take some initial value of alpha and then redunce it such that $f(x+ \\alpha p) < f(x)$ This method is also useful in higher dimensions and guarantees sort of that alpha*\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRulPb3pKBA7"
      },
      "source": [
        "# ***Part 5:*** *Behavior of the backtracking linesearch algorithm for different choices of $\\alpha^0$*\r\n",
        "\r\n",
        "$\\alpha^0 \\in {\\{ 1,0.9,0.75,0.6,0.5,0.4,0.25,0.1,0.01 \\}}$\r\n",
        "\r\n",
        "$x^0 = (10,10)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9} $\r\n",
        "\r\n",
        "$\\rho \\ =\\ 0.5\\\\$\r\n",
        "\r\n",
        "$\\gamma \\ =\\ 0.5\\ $\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq5VIuEQ7kaD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "0debb1fc-0500-4301-d87d-65c826956bc6"
      },
      "source": [
        "my_start_x = np.array([10,10])\r\n",
        "my_tol= 10e-9\r\n",
        "\r\n",
        "alpha_arr = np.array([1,0.9,0.75, 0.6,0.5,0.4,0.25,0.1,0.01])\r\n",
        "rho_ = 0.5\r\n",
        "gamma_ = 0.5\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "x_arr_bls = []\r\n",
        "iter_bls = []\r\n",
        "\r\n",
        "for i in alpha_arr:\r\n",
        "\r\n",
        "  x, k = find_minimizer(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,i, rho_, gamma_ )\r\n",
        "  x_arr_bls.append(x)\r\n",
        "  iter_bls.append(k)\r\n",
        "\r\n",
        "\r\n",
        "x_els, k_els = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH) \r\n",
        "\r\n",
        "df = pd.DataFrame(columns=['alpha', 'x_bls', 'f(x_bls)','iterations_bls', 'x_els', 'f(x_els)', 'iterations_els'])\r\n",
        "\r\n",
        "df['alpha'] = alpha_arr\r\n",
        "df['x_bls'] = x_arr_bls\r\n",
        "df['f(x_bls)'] = df['x_bls'].apply(evalf)\r\n",
        "df['iterations_bls'] = iter_bls\r\n",
        "df['x_els'] = str(x_els)\r\n",
        "df['f(x_els)'] = evalf(x_els)\r\n",
        "df['iterations_els'] = k_els\r\n",
        "\r\n",
        "display(df)\r\n",
        "\r\n",
        "print('\\n\\n')\r\n",
        "plt.plot(alpha_arr, iter_bls, label='Backtraking Line search')\r\n",
        "plt.plot(alpha_arr, [k_els]*9, label='Exact Line Search')\r\n",
        "plt.xlabel('Initial Alpha')\r\n",
        "plt.ylabel('Iterations')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>x_bls</th>\n",
              "      <th>f(x_bls)</th>\n",
              "      <th>iterations_bls</th>\n",
              "      <th>x_els</th>\n",
              "      <th>f(x_els)</th>\n",
              "      <th>iterations_els</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00</td>\n",
              "      <td>[2.0, -3.0]</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.90</td>\n",
              "      <td>[2.0000000008, -2.9999999987]</td>\n",
              "      <td>2.330000e-18</td>\n",
              "      <td>10</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.75</td>\n",
              "      <td>[2.000000001862645, -2.9999999969732016]</td>\n",
              "      <td>1.263096e-17</td>\n",
              "      <td>16</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.60</td>\n",
              "      <td>[2.0000000022517996, -2.9999999963408253]</td>\n",
              "      <td>1.846016e-17</td>\n",
              "      <td>24</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>[2.0, -3.0]</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.40</td>\n",
              "      <td>[2.00000000131072, -2.99999999787008]</td>\n",
              "      <td>6.254545e-18</td>\n",
              "      <td>14</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.25</td>\n",
              "      <td>[2.000000001862645, -2.9999999969732016]</td>\n",
              "      <td>1.263096e-17</td>\n",
              "      <td>32</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.10</td>\n",
              "      <td>[2.0000000025462947, -2.9999999958622707]</td>\n",
              "      <td>2.360442e-17</td>\n",
              "      <td>98</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.01</td>\n",
              "      <td>[2.000000002568644, -2.999999995825952]</td>\n",
              "      <td>2.402061e-17</td>\n",
              "      <td>1082</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   alpha                                      x_bls  ...  f(x_els)  iterations_els\n",
              "0   1.00                                [2.0, -3.0]  ...       0.0               1\n",
              "1   0.90              [2.0000000008, -2.9999999987]  ...       0.0               1\n",
              "2   0.75   [2.000000001862645, -2.9999999969732016]  ...       0.0               1\n",
              "3   0.60  [2.0000000022517996, -2.9999999963408253]  ...       0.0               1\n",
              "4   0.50                                [2.0, -3.0]  ...       0.0               1\n",
              "5   0.40      [2.00000000131072, -2.99999999787008]  ...       0.0               1\n",
              "6   0.25   [2.000000001862645, -2.9999999969732016]  ...       0.0               1\n",
              "7   0.10  [2.0000000025462947, -2.9999999958622707]  ...       0.0               1\n",
              "8   0.01    [2.000000002568644, -2.999999995825952]  ...       0.0               1\n",
              "\n",
              "[9 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZhtbr-y9CV2"
      },
      "source": [
        "***Remarks :*** *We observe that the number of iterations performed by ELS is always 1 and is always less than equal to that of performed by the BLS. Also the lesset the value of the initial alpha the more iterations it takes to find the optimal value of the function. Since the given function f is a quadratic function, ELS has an advantage over BLS in calculating the optimal value faster. When the step size is in fact 0.5, even BLS calculates the answer in 1 iteration. The optimal value of x is (2, -3) and the value of the function at this point is zero*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afOx1Aq999jJ"
      },
      "source": [
        "# ***Part 6:*** *Behavior of the backtracking linesearch algorithm for different choices of $\\rho$*\r\n",
        "\r\n",
        "$\\alpha^0 = 1$\r\n",
        "\r\n",
        "$x^0 = (10,10)$\r\n",
        "\r\n",
        "$\\tau = 10^{-9} $\r\n",
        "\r\n",
        "$\\rho \\in \\{0.9, 0.8, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01\\} $\r\n",
        "\r\n",
        "$\\gamma =0.5$\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "5hi3N8oI8-Eq",
        "outputId": "7967d311-1cb0-43fe-b652-f4e7bb8f5793"
      },
      "source": [
        "my_start_x = np.array([10,10])\r\n",
        "my_tol= 10e-9\r\n",
        "\r\n",
        "alpha_initial = 1 \r\n",
        "rho_arr = np.array([0.9, 0.8, 0.75, 0.6,0.5,0.4,0.25,0.1,0.01])\r\n",
        "gamma_ = 0.5\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "x_arr_bls = []\r\n",
        "iter_bls = []\r\n",
        "\r\n",
        "for i in rho_arr:\r\n",
        "\r\n",
        "  x, k = find_minimizer(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha_initial,i, gamma_ )\r\n",
        "  x_arr_bls.append(x)\r\n",
        "  iter_bls.append(k)\r\n",
        "\r\n",
        "\r\n",
        "x_els, k_els = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH) \r\n",
        "\r\n",
        "df = pd.DataFrame(columns=['rho', 'x_bls', 'f(x_bls)','iterations_bls', 'x_els', 'f(x_els)', 'iterations_els'])\r\n",
        "\r\n",
        "df['rho'] = rho_arr\r\n",
        "df['x_bls'] = x_arr_bls\r\n",
        "df['f(x_bls)'] = df['x_bls'].apply(evalf)\r\n",
        "df['iterations_bls'] = iter_bls\r\n",
        "df['x_els'] = str(x_els)\r\n",
        "df['f(x_els)'] = evalf(x_els)\r\n",
        "df['iterations_els'] = k_els\r\n",
        "\r\n",
        "display(df)\r\n",
        "\r\n",
        "print('\\n\\n')\r\n",
        "plt.plot(rho_arr, iter_bls, label='Backtraking Line search')\r\n",
        "plt.plot(rho_arr, [k_els]*9, label='Exact Line Search')\r\n",
        "plt.xlabel('Rho')\r\n",
        "plt.ylabel('Iterations')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rho</th>\n",
              "      <th>x_bls</th>\n",
              "      <th>f(x_bls)</th>\n",
              "      <th>iterations_bls</th>\n",
              "      <th>x_els</th>\n",
              "      <th>f(x_els)</th>\n",
              "      <th>iterations_els</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.90</td>\n",
              "      <td>[2.000000002322482, -2.999999996225967]</td>\n",
              "      <td>1.963725e-17</td>\n",
              "      <td>7</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.80</td>\n",
              "      <td>[2.0000000017646946, -2.9999999971323716]</td>\n",
              "      <td>1.133744e-17</td>\n",
              "      <td>13</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.75</td>\n",
              "      <td>[2.0000000016940658, -2.999999997247143]</td>\n",
              "      <td>1.044808e-17</td>\n",
              "      <td>12</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.60</td>\n",
              "      <td>[2.0000000008952297, -2.9999999985452517]</td>\n",
              "      <td>2.917729e-18</td>\n",
              "      <td>18</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>[2.0, -3.0]</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.40</td>\n",
              "      <td>[2.00000000131072, -2.99999999787008]</td>\n",
              "      <td>6.254545e-18</td>\n",
              "      <td>14</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.25</td>\n",
              "      <td>[2.000000001862645, -2.9999999969732016]</td>\n",
              "      <td>1.263096e-17</td>\n",
              "      <td>32</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.10</td>\n",
              "      <td>[2.0000000025462947, -2.9999999958622707]</td>\n",
              "      <td>2.360442e-17</td>\n",
              "      <td>98</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.01</td>\n",
              "      <td>[2.000000002568644, -2.999999995825952]</td>\n",
              "      <td>2.402061e-17</td>\n",
              "      <td>1082</td>\n",
              "      <td>[ 2. -3.]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    rho                                      x_bls  ...  f(x_els)  iterations_els\n",
              "0  0.90    [2.000000002322482, -2.999999996225967]  ...       0.0               1\n",
              "1  0.80  [2.0000000017646946, -2.9999999971323716]  ...       0.0               1\n",
              "2  0.75   [2.0000000016940658, -2.999999997247143]  ...       0.0               1\n",
              "3  0.60  [2.0000000008952297, -2.9999999985452517]  ...       0.0               1\n",
              "4  0.50                                [2.0, -3.0]  ...       0.0               1\n",
              "5  0.40      [2.00000000131072, -2.99999999787008]  ...       0.0               1\n",
              "6  0.25   [2.000000001862645, -2.9999999969732016]  ...       0.0               1\n",
              "7  0.10  [2.0000000025462947, -2.9999999958622707]  ...       0.0               1\n",
              "8  0.01    [2.000000002568644, -2.999999995825952]  ...       0.0               1\n",
              "\n",
              "[9 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLiuZcgRAeHk"
      },
      "source": [
        "***Remarks :*** *We observe that as $\\rho$ decreases the number of iterations performed increases. Because the update in the value of $\\alpha$ is determined by $\\rho$. If the value of $\\rho$ is less, $\\alpha$ updates very slowly. Also since the f(x) here is a qudratic function, we see that ELS outperforms BLS since we can get the perfect closed form solution for the step size in ELS*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwgjZM5pVv-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}